{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1655b6a8-efb5-4c8d-8f42-e29904e156ff",
   "metadata": {},
   "source": [
    "# Building a neural net with (just) numpy\n",
    "\n",
    "In this notebook, I aim to build a neural net with just numpy. This means in practice that I will implement the following mathemetical concepts:\n",
    "* matrix multiplication (just using numpy)\n",
    "* differentiation\n",
    "\n",
    "These in turn will provide the foundations for higher level concepts such as:\n",
    "* feedforward pass\n",
    "* loss function calculation\n",
    "* backpropagation\n",
    "\n",
    "The following type of artificial neurons will be coded\n",
    "* linear model\n",
    "* Relu\n",
    "* Mean Squared Error (MSE)\n",
    "* Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79b062-4ec5-46e8-bc71-ccaa8fcaf0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing our core libraries\n",
    "import numpy as np\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b9955-9055-4d51-87ab-9b52df9b381c",
   "metadata": {},
   "source": [
    "# Dataset selection - MNIST\n",
    "\n",
    "We will run our neural network against the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset. MNIST is a large dataset of handwritten digits. It's commonly used as a starting point for people learning about neural networks. \n",
    "\n",
    "These days MNIST is considered a toy dataset because a simple, non-deep network can get you over 99% accuracy. Nevertheless, it is the perfect starting point for our experiments.\n",
    "\n",
    "We will also try to start from scratch here and import the gzipped dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db1be7-76b7-40db-8741-7f6ff9ff27d0",
   "metadata": {},
   "source": [
    "## Downloading MNIST (Training set only)\n",
    "\n",
    "In this section we're only going to import the MNIST training set. \n",
    "We will import the test set a bit later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3911ed-6fa2-49a7-8ec1-39c50e5178d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c201ed4-b9a3-4997-8768-92e0118338db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(url: str, target_file_name: str):\n",
    "    urlretrieve(url, target_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2903f9bf-4a51-4cab-b78a-92fffb2f64e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_images_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz\"\n",
    "mnist_train_images_file_name = \"data/mnist_train_data.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9186f0e-e7de-469d-a0d1-0399bf5ffcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset(mnist_train_images_url, mnist_train_images_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da79bb-fb99-4ed1-8032-325ebb45019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 784 pixels in total per image\n",
    "image_w = 28\n",
    "image_h = 28\n",
    "full_image_dimensions = image_w * image_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f4f05-4e19-492f-8a11-63d2e3a5f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(file_path: str, w, h):\n",
    "    \"\"\"\n",
    "    Loads the images from the dataset into a numpy array\n",
    "    \"\"\"\n",
    "    with gzip.open(file_path, \"rb\") as f:\n",
    "        raw = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "\n",
    "    return raw.reshape(-1, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993c26c-9967-45f8-a85f-8291df632d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = load_images(mnist_train_images_file_name, image_w, image_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55687dbc-51cc-410d-addc-a4d3b4f12266",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c012826-e651-402b-94a3-cc16a61dd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_x[0]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090d0ac-319e-4ccb-8d09-956aaf270c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(file_path: str):\n",
    "    \"\"\"\n",
    "    Load the labels from the file path\n",
    "    \"\"\"\n",
    "    with gzip.open(file_path, \"rb\") as f:\n",
    "        raw = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc090232-a356-4356-9d32-7df7eb24ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_labels_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/train-labels-idx1-ubyte.gz\"\n",
    "mnist_train_labels_file_name = \"data/mnist_train_labels.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94060f79-3955-49fa-a9b7-fb1fba5f7f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset(mnist_train_labels_url, mnist_train_labels_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be86b90-b3ff-437f-822c-55ce4b108752",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = load_labels(mnist_train_labels_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b59cad-87aa-4af4-ae64-8e11b92e89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321e633-632d-46d1-afd6-89d0af2b1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the unique values in the labels - there should be 10 (from 0 to 9)\n",
    "np.unique(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d46b6b-d421-4ca0-b510-84c5e27108de",
   "metadata": {},
   "source": [
    "## Visualising MNIST data\n",
    "\n",
    "Let's see what our dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e078d11-043c-42a2-aa21-5afc7992c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d48ca-583a-4288-a21a-8feb7ab35d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_data(imgs, labels, rows: int, cols: int):\n",
    "    \"\"\"\n",
    "    Plots a grid-like visualisation of the data\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(8,8))\n",
    "\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        # randomly sample from the dataset\n",
    "        sampled_idx = np.random.randint(0, len(imgs))\n",
    "        img, label = imgs[sampled_idx], labels[sampled_idx]\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        plt.title(f\"{label}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ddb10-b8a1-4d8e-a4b6-68c1bb1a8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_data(train_x, train_y, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191892ed-128e-4c93-87e2-c7de9ba52934",
   "metadata": {},
   "source": [
    "## Splitting training and validation data\n",
    "\n",
    "We're going to take 20% of the training data and keep it as the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218be06f-4c30-4b44-b0fa-8f06cc90f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_and_val_size = train_x.shape[0]\n",
    "train_size = int(total_train_and_val_size * 0.8)\n",
    "val_size = total_train_and_val_size - train_size\n",
    "\n",
    "print(f\"{total_train_and_val_size=} | {train_size=} | {val_size=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50adf89-4801-4e81-bede-defed70b96dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick the indices for the elements that will make up our validation set at random\n",
    "val_size_indices = np.random.choice(range(total_train_and_val_size), size=val_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b91a3-4760-4776-891a-0a96aa1c9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_size_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c5291-6c38-4170-affd-b42c34f0ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = train_x[val_size_indices]\n",
    "val_y = train_y[val_size_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2863d52-7cae-43cd-94f6-361192220105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show some of the data from the validation set\n",
    "visualise_data(val_x, val_y, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af031c-116e-4aea-8c35-43904da574ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now remove elements from the training set out of the validation set\n",
    "train_x = np.delete(train_x, val_size_indices, axis=0)\n",
    "train_y = np.delete(train_y, val_size_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e18b26-b2a5-47ee-8450-abbc4fb647f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601159a8-76b0-4def-a08b-7209579527f9",
   "metadata": {},
   "source": [
    "# Building a neural network\n",
    "\n",
    "Our goal is to build a neural network that classifies an input image according to 10 classes representing the digits from 0 to 9. \n",
    "We'll start with a single linear layer that will predict attempt to predict the correct class.\n",
    "\n",
    "We'll also implement ReLU and a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76866ca7-16fb-4685-9d12-5048cbab31f8",
   "metadata": {},
   "source": [
    "## A simple linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f12ab-1925-46f6-9d8d-31ba755ebed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, w_dim: int, b_dim: int):\n",
    "        super().__init__()\n",
    "        # Initialise our weight and bias matrices\n",
    "        self.__w = np.random.rand(w_dim[0], w_dim[1])\n",
    "        self.__b = np.random.rand(b_dim)\n",
    "\n",
    "    def forward(self, x: np.array):\n",
    "        # Keep the x - we will need it for differentiation later\n",
    "        self.__x = x\n",
    "\n",
    "        # We are reshaping our component matrices so that they are compatible\n",
    "        # by default x is of shape (batch_size, 784) and w is of shape (N, 784)\n",
    "        # we turn w into a matrix of dimensions (1, N, 784) and x adopts dimensions (batch_size, 784, 1)\n",
    "        # this makes our matrices compatible because numpy will use broadcasting to perform the matrix multiplication\n",
    "        # This will result in a matrix of dimensions (batch_size, N, 1)\n",
    "        self.__w_x = self.__w[None,] @ self.__x[:, :, None]\n",
    "\n",
    "        # Remember that b is a vector of just N elements\n",
    "        # Remove the last dimension if it is one\n",
    "        self.__z = (self.__w_x + self.__b[:,None]).squeeze(axis=-1)\n",
    "\n",
    "        return self.__z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0c065-9ccc-4aeb-9a7c-662d53872177",
   "metadata": {},
   "source": [
    "In order to be able to output a vector z with 10 columns (basically predictions for digits from 0 to 9), we need to flatten our input parameter `x` from dimensions `28 x 28` to a single vector to dimensions `784`.\n",
    "\n",
    "Our weight matrix `W` thus will have dimensions `10 x 784`. We will perform some clever dimensional reshaping so that we can achieve a `10 x 1` output matrix for the product between `W` and `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a8d06a-5a4d-49a9-97fe-b47c7d612c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.rand(10, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f62ce0f-9bc2-4783-b7f1-178eea6b07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07830a-50eb-435e-b23f-fd2dd3bb9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w[None,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945489e-ba45-4c29-9eaa-22a15fa58db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(w[None,] @ train_x.reshape(-1, 784)[:, :, None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187620ef-a858-4a19-8060-da9ecfd6be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = Linear((10, 784), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415d4d9-53e4-489e-a380-22b5cb29a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_out = lin.forward(train_x.reshape(-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813c004-7cf2-46f5-9f8a-1f5558e6c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effd26d2-ff40-4792-b325-14465f441889",
   "metadata": {},
   "source": [
    "## A simple ReLU layer\n",
    "\n",
    "Next we're going to implement a simple relu layer to clip negative activations to 0 and improve convergence to a solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa46278-dbaa-4e47-b7d5-1f32860207a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.__x = x\n",
    "        self.__relu_x = self.__x.copy()\n",
    "        self.__relu_x[self.__relu_x < 0 ] = 0\n",
    "\n",
    "        return self.__relu_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf94ac-db92-4303-bdad-7b792f882974",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f0963-f8ef-4a0c-a2bb-07bf94e60a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_out = rel.forward(lin_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd734180-9287-44d7-8bf4-953628550a30",
   "metadata": {},
   "source": [
    "## A MSE loss layer\n",
    "\n",
    "The final layer is that of our loss function. Here we are starting out with the Mean Squared Error, which is relatively easy to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0b6ce-e5fe-40d3-9ca0-ff701158b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.__x = x\n",
    "        self.__y = y\n",
    "\n",
    "        self.__loss = np.mean((self.__x - self.__y) ** 2)\n",
    "        return self.__loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47db4d-2de8-4495-8306-54d6edc0febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to expand our labels vector so that it has 10 columns - this is called one-hot encoding\n",
    "train_y_expanded = np.zeros((train_y.shape[0], 10))\n",
    "train_y_expanded[np.arange(train_y.shape[0]),train_y] = 1\n",
    "train_y_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd2e2b-96e3-4876-b600-1cab11793d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to expand our labels vector so that it has 10 columns - this is called one-hot encoding\n",
    "val_y_expanded = np.zeros((val_y.shape[0], 10))\n",
    "val_y_expanded[np.arange(val_y.shape[0]), val_y] = 1\n",
    "val_y_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eaf58f-717b-4ffd-9119-77e95ff7e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check a couple of values to make sure that our 1s are at the correct indices in train_y_expanded\n",
    "train_y_expanded[0:4], train_y[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ab3fd-2234-413b-a281-a41ef95f017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d38a2-6357-42c5-977e-906ac076f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try out our MSE\n",
    "mse = MSE()\n",
    "loss = mse.forward(rel_out, train_y_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec0369-f0b9-4664-a660-42e82a34893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06708ef-3ced-4ff1-8f1f-8eeeb76b3216",
   "metadata": {},
   "source": [
    "## Implementing gradient descent\n",
    "\n",
    "Right now we are able to perform the forward pass, but we're missing the critical pieces required to provide feedback to our network so that it may learn how to make better predictions. To do so we need to find a way to update the weights of our layers.\n",
    "\n",
    "To find a way to update the weights of our layer, we need to compute the partial derivatives with respect to the input and output terms for each layer, and then update the weights in the opposite direction to the gradient. This is in essence what the backpropagation algorithm does.\n",
    "\n",
    "The intuitive reasoning behind this, is because we can think of a neural network as a series of compositions of functions... From a notation standpoint we could write:\n",
    "\n",
    "$$\n",
    "\\text{Network} = MSE(ReLU(Linear(x, w, b)))\n",
    "$$\n",
    "\n",
    "The above is a mathematical definition of the _forward_ pass of the network.\n",
    "\n",
    "\n",
    "Now, assuming the functions in the expression above are differentiable (they are), we can find a way to determine how changing specific parameters affects the output. This is after all what differentiation allows you to discover. \n",
    "\n",
    "Remember also that the formula for calculating the derivative of a function composition is the following:\n",
    "\n",
    "$$\n",
    "\\text{The chain rule states: If } y = f(u) \\text{ and } u = g(x), \\text{ then the derivative of } y \\text{ with respect to } x \\text{ is:}\n",
    "$$\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} \n",
    "$$\n",
    "\n",
    "Put more succinctly, we could also write:\n",
    "$$\n",
    "\\frac{d}{dx}h(x) = \\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)\n",
    "$$\n",
    "\n",
    "\n",
    "There is one little we forgot to mention... We are dealing with a multi-dimensional problem that involves multiple variables. So we need to understand how changing each variable affects the overall result.\n",
    "\n",
    "We do this by computing not a single derivative, but instead by calculating the partial derivatives with respect to a given term (e.g. with respect to $w$ or $b$, etc.).\n",
    "\n",
    "So essentially, we apply the chain rule recursively (from the last layer of the network up until the first layer) by computing the partial derivative for the appropriate terms. This is called the _backward_ pass, since it runs in the opposite direction to the forward pass.\n",
    "\n",
    "Well... that's exactly what backpropagation is! \n",
    "\n",
    "\n",
    "Let's thus reimplement our classes with a backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d082085-fdf4-4ed7-ad61-c03bc1c54472",
   "metadata": {},
   "source": [
    "### MSE Loss layer with gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d718f-977a-46ef-b2d0-d589065e72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEWithGrad():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.__x = x\n",
    "        self.__y = y\n",
    "\n",
    "        self.__diff = self.__x - self.__y\n",
    "        self.__loss = np.mean(self.__diff ** 2)\n",
    "        return self.__loss\n",
    "\n",
    "    def preds(self):\n",
    "        return self.__diff ** 2\n",
    "\n",
    "    def backward(self):\n",
    "        self.__grad = 2 * self.__diff / self.__diff.shape[0]\n",
    "\n",
    "    def gradient(self):\n",
    "        return self.__grad\n",
    "\n",
    "    def gradient_update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        MSE doesn't have learnable parameters so there is nothing to update in gradient descent\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def gradient_zero(self):\n",
    "        self.__grad = 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33725112-a539-4aa0-a9db-094855d216f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_w_grad = MSEWithGrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4d94b-5c3e-4f24-bb20-16592aa2ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse_w_grad.forward(rel_out, train_y_expanded)\n",
    "mse_w_grad.backward()\n",
    "mse_gradient = mse_w_grad.gradient()\n",
    "\n",
    "print(f\"{loss=} | {mse_gradient=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c6aba-ff99-41d8-9959-bb783816a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_gradient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f297e61-0b55-4f77-84e1-8770dd4dcb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluWithGrad():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.__x = x\n",
    "        self.__relu_x = self.__x.copy()\n",
    "        self.__relu_x[self.__relu_x < 0 ] = 0\n",
    "\n",
    "        return self.__relu_x\n",
    "\n",
    "    def backward(self, next_layer_grad):\n",
    "        \"\"\"\n",
    "        Computes gradient of this layer using backpropagation\n",
    "        \"\"\"\n",
    "        x_copy = self.__x.copy()\n",
    "        self.__d_relu_dx = np.where(x_copy > 0, 1, 0)\n",
    "        self.__grad = self.__d_relu_dx * next_layer_grad\n",
    "\n",
    "    def gradient(self):\n",
    "        return self.__grad\n",
    "\n",
    "    def gradient_update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        ReLU doesn't have learnable parameters so there is nothing to update in gradient descent\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def gradient_zero(self):\n",
    "        self.__d_relu_dx = 0.\n",
    "        self.__grad = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0d3e2-f50b-4149-a0bc-3f018a4dfb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_w_grad = ReluWithGrad()\n",
    "relu_w_grad_out = relu_w_grad.forward(lin_out)\n",
    "relu_w_grad.backward(mse_gradient)\n",
    "relu_w_grad_gradient = relu_w_grad.gradient()\n",
    "\n",
    "print(f\"{relu_w_grad_out.shape=} | {mse_gradient.shape=} | {relu_w_grad_gradient=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd980c6e-00dc-4c3c-a4c4-259b7fab721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithGrad():\n",
    "    def __init__(self, w_dim: int, b_dim: int):\n",
    "        super().__init__()\n",
    "        # Initialise our weight and bias matrices\n",
    "        self.__w = np.random.rand(w_dim[0], w_dim[1])\n",
    "        self.__b = np.random.rand(b_dim)\n",
    "\n",
    "        print(f\"{self.__w.shape=} | {self.__b.shape=}\")\n",
    "\n",
    "    def forward(self, x: np.array):\n",
    "        # Keep the x - we will need it for differentiation later\n",
    "        self.__x = x\n",
    "        # print(f\"{self.__x.shape=}\")\n",
    "\n",
    "        # We are reshaping our component matrices so that they are compatible\n",
    "        # by default x is of shape (batch_size, 784) and w is of shape (10, 784)\n",
    "        # we turn w into a matrix of dimensions (1, 10, 784) and x adopts dimensions (batch_size, 784, 1)\n",
    "        # this makes our matrix compatible because numpy will use broadcasting to perform the matrix multiplication\n",
    "        # This will result in a matrix of dimensions (batch_size, 10, 1)\n",
    "        self.__w_x = self.__x @ self.__w\n",
    "\n",
    "        # Remember that b is a vector of just 10 elements\n",
    "        # Remove the last dimension if it is one\n",
    "        # self.__z = (self.__w_x + self.__b[None,:]).squeeze(axis=-1)\n",
    "        self.__z = (self.__w_x + self.__b[None,:])\n",
    "\n",
    "        # print(f\"{self.__z.shape=}\")\n",
    "\n",
    "        return self.__z\n",
    "\n",
    "    def backward(self, next_layer_grad):\n",
    "        self.__d_lin_dx = next_layer_grad @ self.__w.T\n",
    "        self.__d_lin_dw = self.__x.T @ next_layer_grad\n",
    "        self.__d_lin_db = next_layer_grad.sum(axis=0)\n",
    "\n",
    "    def gradient(self):\n",
    "        return self.gradient_inputs()\n",
    "\n",
    "    def gradient_inputs(self):\n",
    "        return self.__d_lin_dx\n",
    "\n",
    "    def gradient_weights(self):\n",
    "        return self.__d_lin_dw\n",
    "\n",
    "    def gradient_bias(self):\n",
    "        return self.__d_lin_db\n",
    "\n",
    "    def gradient_update(self, learning_rate):\n",
    "        self.__w -= self.__d_lin_dw * learning_rate\n",
    "        self.__b -= self.__d_lin_db * learning_rate\n",
    "\n",
    "    def gradient_zero(self):\n",
    "        self.__d_lin_dw = 0\n",
    "        self.__d_lin_db = 0\n",
    "        self.__d_lin_dx = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33a83a-2a1f-431a-9c72-6d17e155a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_w_grad = LinearWithGrad((784, 10), 10)\n",
    "lin_w_grad_out = lin_w_grad.forward(train_x.reshape(-1, 784))\n",
    "lin_w_grad.backward(relu_w_grad_gradient)\n",
    "\n",
    "lin_w_grad_gradient_weights = lin_w_grad.gradient_weights()\n",
    "lin_w_grad_gradient_bias = lin_w_grad.gradient_bias()\n",
    "\n",
    "print(f\"{lin_w_grad_gradient_weights.shape=} | {lin_w_grad_gradient_bias.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2349b-0912-46af-b828-8b1cae918c3b",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Let's train our neural network from scratch using everything we built up until now. But first we will need to carry out a bit of pre-processing on our MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d35730-5501-4bb4-b82d-7b61c148c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mnist(x: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    In this step we simply normalize the values so that\n",
    "    they range from [0-1] instead of [0-255].\n",
    "    This will improve conversion\n",
    "    \"\"\"\n",
    "    return x/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec3535e-1b80-4a4c-b985-f6caa9e68d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(preds_y, y) -> float:\n",
    "    \"\"\"\n",
    "    Returns percentage of correct predictions across the whole dataset\n",
    "    \"\"\"\n",
    "    total_examples = y.shape[0]\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for i in range(total_examples):\n",
    "        highest_pred = np.argmax(preds_y[i])\n",
    "        ground_truth = np.argmax(y[i])\n",
    "\n",
    "        if highest_pred == ground_truth:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    return float(correct_predictions) / float(total_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24401f00-0dfe-4836-b62f-64b5471a734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "epochs = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30622a0-75d1-49f8-854e-c0c416296099",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [LinearWithGrad((784, 10), 10), ReluWithGrad(), MSEWithGrad()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19d51e-d16a-4523-9f15-c96b8168613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = preprocess_mnist(train_x.reshape(-1, 784))\n",
    "ground_truth = train_y_expanded\n",
    "out = None\n",
    "for i in range(epochs):\n",
    "    inp = input\n",
    "    for m_i, m in enumerate(model):\n",
    "        if m_i == len(model) -1 :\n",
    "            out = m.forward(inp, ground_truth)\n",
    "            losses.append(out)\n",
    "        else:\n",
    "            out = m.forward(inp)\n",
    "        inp = out\n",
    "\n",
    "    grad = None\n",
    "    for m_i, m in enumerate(reversed(model)):\n",
    "        if m_i == 0:\n",
    "            m.backward()\n",
    "        else:\n",
    "            m.backward(grad)\n",
    "\n",
    "        grad = m.gradient()\n",
    "\n",
    "    for m in model:\n",
    "        m.gradient_update(learning_rate)\n",
    "        m.gradient_zero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f3c88-a769-43a0-b5b4-2a30b396c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8172a-0360-4c64-8d16-2ad4d19b6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualise our loss...\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f4f7d-2a41-4cab-95f0-fe9ac87c0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylabel(\"loss score\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b50a18e-0c0f-447a-978d-0f9f7fc39779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the output looks like\n",
    "preds = model[-1].preds()\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2044be1a-c374-4d8f-932c-9743a8a120d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "preds[n], np.argmax(preds[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e570db-4b5e-43db-a5d2-32ca8985ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmax(preds[n])\n",
    "true_pred = np.argmax(train_y_expanded[n])\n",
    "\n",
    "idx, true_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f463e2a6-c66f-458c-8dd0-465feb232838",
   "metadata": {},
   "source": [
    "# Building a Softmax and Cross Entropy Loss layers\n",
    "\n",
    "Since we are dealing with a classification problem, using Softmax the towards the end of our network is a much more appropriate choice since we can better interpret the predictions of the network with respect to the possible values of _Y_ (i.e. digits from 0 to 9).\n",
    "\n",
    "$$ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} $$\n",
    "\n",
    "Additionally, we will be using the cross entropy loss function. Let's explain the formula for cross-entropy loss below:\n",
    "\n",
    "$$\n",
    "L(y, p) = - {\\sum_{j=1}^{K} y_j  log(p(y_j))}\n",
    "$$\n",
    "\n",
    "Here $y_j$ represents the ground truth probability a given class, while $p(y_j)$ represents the predicted (by our network) probability for each label. In our case, we are dealing with a typical classification problem, where there can only be one correct class $y_i$ per example. \n",
    "\n",
    "Therefore it follows that $y_i = 1$.\n",
    "Additionally, it also follows that $y_k = 0, k \\neq i$. **This means that all ground truth probabilities for incorrect classes are 0**. So our loss function can effectively be simplified, where for each example \n",
    "\n",
    "$$\n",
    "L(y, p) = - y_i  log(p(y_i))\n",
    "$$\n",
    "\n",
    "Since $y_i = 1$ the formula simply becomes:\n",
    "\n",
    "$$\n",
    "L(y, p) = - log(p(y_i))\n",
    "$$\n",
    "\n",
    "Interestingly, this is a special case of cross entropy called the **Negative Loss Likelihood**. Although both terms are often used interchangibly in ML circles, negative loss likelihood only makes sense if the probabilities for each correct label is 1, while those of the incorrect labels are all 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0dc186-cb98-49ef-8c62-ce181c50170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.__numerator = np.exp(z)\n",
    "        self.__denominator = np.sum(self.__numerator, axis=1, keepdims=True)\n",
    "        self.__softmax = self.__numerator / self.__denominator\n",
    "\n",
    "        return self.__softmax\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea0bea-1b1d-4ac6-9c21-1fc2383d7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        self.__y_i_index = np.argmax(y, axis=1)\n",
    "        self.__z_i = z[np.arange(z.shape[0]), self.__y_i_index]\n",
    "        self.__loss_per_row = - (np.log(self.__z_i))\n",
    "\n",
    "        # The final loss is the average loss across the whole batch\n",
    "        self.__loss = self.__loss_per_row.mean()\n",
    "        return self.__loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba297d-ab7b-4e06-b7f5-682f0025ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df15e0-a217-45e4-b263-d2a53413f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = Softmax()\n",
    "sm_out = sm.forward(rel_out)\n",
    "sm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df950e3-f6d6-4d18-adc1-621aed5f8f30",
   "metadata": {},
   "source": [
    "## Making softmax more numerically stable\n",
    "\n",
    "What a bummer! Looks like we overflow across most computations of our softmax layer. This is due to raising to the exponential of large numbers (over $10^4$).\n",
    "\n",
    "There's trick to make the computations more numerically stable though... What we can do is shift our numbers to a lower range, which should make the calculations more stable numerically. To do this, we identify the largest number $m_i$ on each row (i.e. for each example) from our input $z$, and instead take the exponential of the difference between the value original value of z and our max number. \n",
    "\n",
    "The formula becomes thus:\n",
    "\n",
    "$$ \n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i - \\text{max}(z)}}{\\sum_{j=1}^{K} e^{z_j-\\text{max}(z)}} \n",
    "$$\n",
    "\n",
    "What's good about this new formula is that the relative order of all inputs is also preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed06ec-2755-45c7-a77f-294e05b95e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxStable:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.__z = z\n",
    "        self.__max_z = np.max(z, axis=1, keepdims=True)\n",
    "        self.__z_minus_max = z - self.__max_z\n",
    "\n",
    "        self.__numerator = np.exp(self.__z_minus_max)\n",
    "        self.__denominator = np.sum(self.__numerator, axis=1, keepdims=True)\n",
    "        self.__softmax = self.__numerator / self.__denominator\n",
    "\n",
    "        return self.__softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad99a6e-09b5-460a-96e6-c51a0a41bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = SoftmaxStable()\n",
    "sms_out = sms.forward(rel_out)\n",
    "sms_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28d99f-63e3-4edb-9df6-9a67996ac45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed8de7-0308-45b0-b72e-de67e0c579d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cel = CrossEntropyLoss()\n",
    "cel_out = cel.forward(sms_out, train_y_expanded)\n",
    "cel_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e23e4b-be1b-4f5f-a77f-9faec5f3f04b",
   "metadata": {},
   "source": [
    "## Making Cross Entropy Loss more numerically stable\n",
    "\n",
    "I've just hit another roadblock 🫠. I get some infinity errors because some of the values from the softmax layer (the ones for the true probability class) are $0$ or very very close to it. **For recall $log(0)$ is undefined**, which is probably why I'm getting this issue.\n",
    "\n",
    "What we can do to address this issue is to add a constant term _epsilon_, which will ensure that the outputs of my softmax later are never truly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb2df0-34fc-465a-a0e1-110d306f45ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossStable:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.__epsilon = 1e-9\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        self.__y_i_index = np.argmax(y, axis=1)\n",
    "        self.__z_i = z[np.arange(z.shape[0]), self.__y_i_index]\n",
    "        self.__z_epislon = self.__z_i + self.__epsilon\n",
    "        self.__loss_per_row = - (np.log(self.__z_epislon))\n",
    "\n",
    "        # The final loss is the average loss across the whole batch\n",
    "        self.__loss = self.__loss_per_row.mean()\n",
    "        return self.__loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b88e4c-e364-4a73-85d1-7890ecee9550",
   "metadata": {},
   "outputs": [],
   "source": [
    "cels = CrossEntropyLossStable()\n",
    "cels_out = cels.forward(sms_out, train_y_expanded)\n",
    "cels_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be13a84-036e-47c0-9344-51588bc726f4",
   "metadata": {},
   "source": [
    "# Putting it all together (2)\n",
    "\n",
    "Now that we can compute the Softmax and Cross Entropy, we need to implement backpropagation logic for these two layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d15859-2e6a-4ba7-808b-765a335ec0da",
   "metadata": {},
   "source": [
    "## Gradient of cross entropy layer\n",
    "\n",
    "The gradient of the cross entropy loss function is simply the difference between the predicted loss batch and the ground truth for each item in the batch. From a notation standpoint, we could write the following:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_i} = p_i - y_i\n",
    "$$\n",
    "\n",
    "Let's explain this formula in more detail:\n",
    "* $\\mathcal{L}$ represents the cross entropy loss function\n",
    "* $z_i$ represents the logits; i.e. inputs into the Softmax function\n",
    "* $p_i$ is the predicted probabilities for class _i_, which is produced from the Softmax function\n",
    "* $y_i$ is the ground truth; i.e. the actual probability for class $i$ (either 1 or 0 depending on whether $i$ is the true class)\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial z_i}$ is thus the partial derivative of the loss function with respect to the logit $z_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec73db-f3de-406e-a17d-9ea4c4564501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossStableWithGradient:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.__epsilon = 1e-9\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        self.__y = y\n",
    "        self.__z = z\n",
    "        self.__y_i_index = np.argmax(y, axis=1)\n",
    "        self.__z_i = z[np.arange(z.shape[0]), self.__y_i_index]\n",
    "        self.__z_epislon = self.__z_i + self.__epsilon\n",
    "        self.__loss_per_row = - (np.log(self.__z_epislon))\n",
    "\n",
    "        # The final loss is the average loss across the whole batch\n",
    "        self.__loss = self.__loss_per_row.mean()\n",
    "        return self.__loss\n",
    "\n",
    "    def backward(self):\n",
    "        self.__d_loss_z = self.__z - self.__y\n",
    "\n",
    "    def gradient(self):\n",
    "        return self.__d_loss_z\n",
    "\n",
    "    def gradient_update(self, learning_rate):\n",
    "        # Nothing to update since Cross Entropy doesn't have hidden layers\n",
    "        pass\n",
    "\n",
    "    def preds(self):\n",
    "        return self.__z\n",
    "\n",
    "    def gradient_zero(self):\n",
    "        self.__d_loss_z = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380266bd-b93a-462d-8b50-da1c9b8d32cb",
   "metadata": {},
   "source": [
    "## Gradient of Softmax layer\n",
    "\n",
    "Calculating the gradient of the Softmax layer is a lot more involved. We could use a little trick to simplify it, assuming that Softmax is combined with cross entropy at the end. But for the purpose of this exercise, we won't make this assumption.\n",
    "\n",
    "The first thing we need to do is compute the **Jacobian** for the Softmax layer. The Jacobian represents the matrix of first order partial derivatives. \n",
    "\n",
    "Remember that for any item in the batch, the formula for Softmax for a given class $i$ (assuming $K$ classes) is the following \n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "We can now think about how to calculate the Jacobian for the expression above... Strap on to your seats, because this is going to get exciting.\n",
    "\n",
    "---\n",
    "\n",
    "### Calculating the Jacobian of Softmax\n",
    "\n",
    "Since we are dealing with a batch, we can expand this formula to a matrix form. Let's assume our batch contains 3 examples, and we have 3 classes per batch.\n",
    "\n",
    "In this example, the output of the forward pass for the Softmax layer thus becomes the following matrix:\n",
    "\n",
    "$$\n",
    "\\text{S} = \\begin{pmatrix}\n",
    "\\frac{e^{z_{11}}}{\\sum_{j=1}^{3} e^{z_{1j}}} & \\frac{e^{z_{12}}}{\\sum_{j=1}^{3} e^{z_{1j}}} & \\frac{e^{z_{13}}}{\\sum_{j=1}^{3} e^{z_{1j}}} \\\\\n",
    "\\frac{e^{z_{21}}}{\\sum_{j=1}^{3} e^{z_{2j}}} & \\frac{e^{z_{22}}}{\\sum_{j=1}^{3} e^{z_{2j}}} & \\frac{e^{z_{23}}}{\\sum_{j=1}^{3} e^{z_{2j}}} \\\\\n",
    "\\frac{e^{z_{31}}}{\\sum_{j=1}^{3} e^{z_{3j}}} & \\frac{e^{z_{32}}}{\\sum_{j=1}^{3} e^{z_{3j}}} & \\frac{e^{z_{33}}}{\\sum_{j=1}^{3} e^{z_{3j}}} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We can further adopt a more compact notation for the above matrix with the expression below:\n",
    "$$\n",
    "\\text{S} = \\begin{pmatrix}\n",
    "S_{11} & S_{12} & S_{13} \\\\\n",
    "S_{21} & S_{22} & S_{23} \\\\\n",
    "S_{31} & S_{22} & S_{33} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "For each item in the batch from the formula above, **we can see that we need to compute 3 partial derivatives for any input $z_i$ because we have 3 classes ($K = 3$)** .\n",
    "\n",
    "So this means in practical terms, for input $z_{1i}$ (i.e. first item in the batch), its Jacobian is the following:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_{1}}{\\partial z_{1}} = \\begin{pmatrix}\n",
    "\\frac{\\partial S_{11}}{\\partial z_{11}} & \\frac{\\partial S_{12}}{\\partial z_{11}} & \\frac{\\partial S_{13}}{\\partial z_{11}} \\\\\n",
    "\\frac{\\partial S_{11}}{\\partial z_{12}} & \\frac{\\partial S_{12}}{\\partial z_{12}} & \\frac{\\partial S_{13}}{\\partial z_{12}} \\\\\n",
    "\\frac{\\partial S_{11}}{\\partial z_{13}} & \\frac{\\partial S_{12}}{\\partial z_{13}} & \\frac{\\partial S_{13}}{\\partial z_{13}} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "With the above, it follows that **for any single item in the batch (i.e. a row), its Jacobian is a square matrix of $K \\times K$ dimensions, where K is the number of classes**. So this means that in our example above, the Jacobian for the first item in the batch is a $3 \\times 3$ matrix. Furthermore, it follows that the Jacobian for the full match is a matrix of $B \\times K \\times K$ dimensions. In our example where $B = 3$ and $K = 3$, our Jacobian is a $3 \\times 3 \\times 3$ matrix.\n",
    "\n",
    "---\n",
    "\n",
    "#### Calculating the partial derivatives with respect to one logit\n",
    "\n",
    "To derive a more generic formula for the partial derivatives, let's look at how we may compute the 3 partial derivatives with respect to $z_{11}$. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial S_{11}}{\\partial z_{11}} &= \\frac{\\partial}{\\partial z_{11}} \\frac{e^{z_{11}}}{\\sum_{j=1}^{K} e^{z_{1k}}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To compute such a derivative, we employ the *quotient rule*, expressed in the below:\n",
    "$$\n",
    "\\frac{d}{dx} \\left( \\frac{u}{v} \\right) = \\frac{v \\frac{du}{dx} - u \\frac{dv}{dx}}{v^2}\n",
    "$$\n",
    "\n",
    "\n",
    "##### **First partial derivative**\n",
    "So plugging in all we know, the partial derivative of $S_{11}$ with respect to $z_{11}$ is:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial S_{11}}{\\partial z_{11}} &= \\frac{\\partial}{\\partial z_{11}} \\frac{e^{z_{11}}}{\\sum_{j=1}^{K} e^{z_{1k}}} \\\\\n",
    "&= \\frac{e^{z_{11}} \\cdot (e^{z_{11}} + e^{z_{12}} + e^{z_{13})} - e^{z_{11}} \\cdot e^{z_{11}} }{(e^{z_{11}} + e^{z_{12}} + e^{z_{13})^2}} \\\\\n",
    "&= \\frac{e^{z_{11}} \\cdot [(e^{z_{11}} + e^{z_{12}} + e^{z_{13}}) - e^{z_{11}}] }{(e^{z_{11}} + e^{z_{12}} + e^{z_{13})^2}} \\\\\n",
    "&= \\frac{e^{z_{11}} \\cdot (e^{z_{11}} + e^{z_{12}} + e^{z_{13}})}{(e^{z_{11}} + e^{z_{12}} + e^{z_{13})^2}} - \\frac{e^{z_{11}} \\cdot e^{z_{11}}}{(e^{z_{11}} + e^{z_{12}} + e^{z_{13})^2}} \\\\\n",
    "&= \\frac{e^{z_{11}}}{e^{z_{11}} + e^{z_{12}} + e^{z_{13}}} - (\\frac{e^{z_{11}}}{e^{z_{11}} + e^{z_{12}} + e^{z_{13}}})^2 \\\\\n",
    "&= S_{11} - (S_{11})^2 \\\\\n",
    "&= S_{11} \\cdot (1 - S_{11}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Expression the above as a one-line formula, we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial S_{11}}{\\partial z_{11}} &= S_{11} \\cdot (1 - S_{11}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "##### **Second partial derivative**\n",
    "\n",
    "Now let's move on to the second partial derivative, still with respect to $z_{11}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial S_{12}}{\\partial z_{11}} &= \\frac{\\partial}{\\partial z_{11}} \\frac{e^{z_{12}}}{\\sum_{j=1}^{K} e^{z_{1k}}} \\\\\n",
    "&= \\frac{0 - e^{z_{11}} \\cdot e^{z_{12}} }{(e^{z_{11}} + e^{z_{12}} + e^{z_{13}})^2} \\\\\n",
    "&= - (\\frac{e^{z_{11}}}{(e^{z_{11}} + e^{z_{12}} + e^{z_{13}})} \\cdot \\frac{e^{z_{12}}}{(e^{z_{11}} + e^{z_{12}} + e^{z_{13}})}) \\\\\n",
    "&= - (S_{11} \\cdot S_{12})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Expressing the above as a one-line formula, we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial S_{12}}{\\partial z_{11}} &= - (S_{11} \\cdot S_{12}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "##### **Third partial derivative**\n",
    "\n",
    "Finally, the third partial derivative, still with respect to $z_{11}$, is expressed is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial S_{13}}{\\partial z_{11}} &= \\frac{\\partial}{\\partial z_{11}} \\frac{e^{z_{13}}}{\\sum_{j=1}^{K} e^{z_{1k}}} \\\\\n",
    "&= \\frac{0 - e^{z_{11}} \\cdot e^{z_{13}} }{(e^{z_{11}} + e^{z_{12}} + e^{z_{13}})^2} \\\\\n",
    "&= - (\\frac{e^{z_{11}}}{(e^{z_{11}} + e^{z_{13}} + e^{z_{13}})} \\cdot \\frac{e^{z_{13}}}{(e^{z_{11}} + e^{z_{12}} + e^{z_{13}})}) \\\\\n",
    "&= - (S_{11} \\cdot S_{13})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Expressing the above as a one-line formula, we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial S_{13}}{\\partial z_{11}} &= - (S_{11} \\cdot S_{13}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### General rules for partial derivative\n",
    "\n",
    "We can see from the above that we can derive the partial derivative with respect to any term using the following formulas\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial S_j}{\\partial z_{i}} &= S_{i} \\cdot (1 - S_{i}), i = j \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_j}{\\partial z_{i}} = - (S_{i} \\cdot S_{j}), i \\neq j \\\\\n",
    "$$\n",
    "\n",
    "As result, this means we could rewrite the expression above for the Jacobian of for the first item in our example batch of 3 classes as the following:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_{1}}{\\partial z_{1}} = \\begin{pmatrix}\n",
    "S_{11} \\cdot (1 - S_{11}) & - (S_{11} \\cdot S_{12}) & - (S_{11} \\cdot S_{13}) \\\\\n",
    "-(S_{12} \\cdot S_{11}) & S_{12} \\cdot (1 - S_{12}) & -(S_{12} \\cdot S_{13}) \\\\\n",
    "-(S_{13} \\cdot S_{11}) & -(S_{13} \\cdot S_{12}) & S_{13} \\cdot (1 - S_{13}) \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "See how the diagonal elements match the case where $i=j$, while all off-diagonal elements match the case where $i \\neq j$.\n",
    "\n",
    "With this knowledge, we can go on to code the full Jacobian in our Softmax layer...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd6699-c3ea-4d8a-b3e4-d394d6200ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxStableWithGradient:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.__z = z\n",
    "        self.__max_z = np.max(z, axis=1, keepdims=True)\n",
    "        self.__z_minus_max = z - self.__max_z\n",
    "\n",
    "        self.__numerator = np.exp(self.__z_minus_max)\n",
    "        self.__denominator = np.sum(self.__numerator, axis=1, keepdims=True)\n",
    "        self.__softmax = self.__numerator / self.__denominator\n",
    "\n",
    "        return self.__softmax\n",
    "\n",
    "    def backward(self, next_layer_grad):\n",
    "        # Initialise the Jacobian - it's a 3D array where for each sample in the batch we compute a 2D array\n",
    "        batch_size, num_classes = self.__softmax.shape\n",
    "        self.__d_softmax_z = np.zeros((batch_size, num_classes, num_classes))\n",
    "        # Now iterate through each element to create the Jacobian\n",
    "        for i in range(batch_size):\n",
    "            diag_i = np.diagflat(self.__softmax[i])\n",
    "            jacobian_i =  - np.outer(self.__softmax[i], self.__softmax[i]) + diag_i\n",
    "            self.__d_softmax_z[i] = jacobian_i\n",
    "\n",
    "        self.__gradient = np.einsum(\"ij,ijk->ik\", next_layer_grad,  self.__d_softmax_z)\n",
    "\n",
    "    def preds(self):\n",
    "        return self.__softmax\n",
    "\n",
    "    def gradient(self):\n",
    "        return self.__gradient\n",
    "\n",
    "    def gradient_update(self, learning_rate):\n",
    "        # Nothing to update since softmax doesn't have hidden layers\n",
    "        pass\n",
    "\n",
    "    def gradient_zero(self):\n",
    "        self.__gradient = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ade42-ef21-4ee9-af82-1290ba8aa860",
   "metadata": {},
   "source": [
    "# Putting it all together together\n",
    "\n",
    "Let's run our network again with the softmax and cross entropy layers and see how well it fares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2445ec45-edc8-4b08-adb2-2aeec030b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "epochs = 100\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadeb32d-074d-4496-8ab0-91f4efecebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [LinearWithGrad((784, 10), 10), SoftmaxStableWithGradient(), CrossEntropyLossStableWithGradient()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f94bc-b618-4d13-a9b7-8a5095cbfbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = preprocess_mnist(train_x.reshape(-1, 784))\n",
    "ground_truth = train_y_expanded\n",
    "out = None\n",
    "for i in range(epochs):\n",
    "    inp = input\n",
    "    for m_i, m in enumerate(model):\n",
    "        if m_i == len(model) -1 :\n",
    "            out = m.forward(inp, ground_truth)\n",
    "            losses.append(out)\n",
    "        else:\n",
    "            out = m.forward(inp)\n",
    "        inp = out\n",
    "\n",
    "    grad = None\n",
    "    for m_i, m in enumerate(reversed(model)):\n",
    "        if m_i == 0:\n",
    "            m.backward()\n",
    "        else:\n",
    "            m.backward(grad)\n",
    "\n",
    "        grad = m.gradient()\n",
    "\n",
    "    for m in model:\n",
    "        m.gradient_update(learning_rate)\n",
    "        m.gradient_zero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3202b-3d1c-40d1-9286-8d96fec7e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, learning_rate, x, y) -> (List[float], List[float]):\n",
    "    \"\"\"\n",
    "    Trains the model for the set number of epochs.\n",
    "    Returns a tuple composed of the losses per epoch along with the accuracy per epoch\n",
    "    \"\"\"\n",
    "    input = x\n",
    "    ground_truth = y\n",
    "    losses = []\n",
    "    accuracy = []\n",
    "    out = None\n",
    "    for i in range(epochs):\n",
    "        inp = input\n",
    "        for m_i, m in enumerate(model):\n",
    "            if m_i == len(model) -1 :\n",
    "                # Compute the accuracy with the predictions from the penultimate layer just before we do a forward pass to the last layer\n",
    "                accuracy.append(calculate_accuracy(out, ground_truth))\n",
    "\n",
    "                out = m.forward(inp, ground_truth)\n",
    "                losses.append(out)\n",
    "            else:\n",
    "                out = m.forward(inp)\n",
    "            inp = out\n",
    "\n",
    "        # Now do backpropagation...\n",
    "        grad = None\n",
    "        for m_i, m in enumerate(reversed(model)):\n",
    "            if m_i == 0:\n",
    "                m.backward()\n",
    "            else:\n",
    "                m.backward(grad)\n",
    "\n",
    "            grad = m.gradient()\n",
    "\n",
    "        # Now do another forward pass where we update the layers by applying a learning rate to the gradients\n",
    "        for m in model:\n",
    "            m.gradient_update(learning_rate)\n",
    "            # Don't forget to reset the gradients\n",
    "            m.gradient_zero()\n",
    "\n",
    "    return (losses, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033374e5-f12d-4eeb-b370-2bb2b8850069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x, y) -> np.array:\n",
    "    \"\"\"\n",
    "    Returns the model's predictions for a given input x and ground truth y\n",
    "    \"\"\"\n",
    "    inp = x\n",
    "    out = None\n",
    "    preds = None\n",
    "    ground_truth = y\n",
    "    for m_i, m in enumerate(model):\n",
    "        if m_i == len(model) -1 :\n",
    "            # predictions are made in the penultimate layer, before the loss function layer\n",
    "            preds = out\n",
    "\n",
    "            out = m.forward(inp, ground_truth)\n",
    "            losses.append(out)\n",
    "        else:\n",
    "            out = m.forward(inp)\n",
    "        inp = out\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45a4ea-423d-43a6-92d4-359c922d57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ,\n",
    "train_x_preprocessed = preprocess_mnist(train_x.reshape(-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3155b6-0075-48da-8268-31b182244303",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "m = [LinearWithGrad((784, 10),10), SoftmaxStableWithGradient(), CrossEntropyLossStableWithGradient()]\n",
    "eps = 200\n",
    "ls, accs = train_model(m, eps, lr, train_x_preprocessed, train_y_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0732ed-45fa-48f8-b5a5-50f994e1fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x_preprocessed = preprocess_mnist(val_x.reshape(-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc6e2d-a287-4140-a4d4-5d90c615f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x_preprocessed.shape, val_y_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a483c-dea4-453e-83ee-e3a33d0ba933",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = predict(m, val_x_preprocessed, val_y_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27a14e-fee0-4720-8c1b-683d4b72b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accs = calculate_accuracy(val_preds, val_y_expanded)\n",
    "val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683a8c3-8fb4-4ec2-85f8-23c0fc9116b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efecf41-8a99-42d9-854b-2611f0ebb1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dcf4fb-9a66-4306-9284-3d309eae1bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6198d1-fd28-4405-8100-51ce0e3d2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ls)\n",
    "plt.ylabel(\"loss score\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5dd06-099f-47f5-ba3b-8998a4cd9f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accs)\n",
    "plt.ylabel(\"accuracy (train set) (%)\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5444222-83fc-473b-a673-3c8835cdce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6936ac9a-401c-4afe-9e20-9430fa45bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the output looks like\n",
    "preds = model[-1].preds()\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7fed0-0135-4ecd-ad2e-fabbdaff0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1500\n",
    "preds[n], np.argmax(preds[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec248e-ce1b-4a17-aba5-33460fef8758",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmax(preds[n])\n",
    "true_pred = np.argmax(train_y_expanded[n])\n",
    "\n",
    "idx, true_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1972a3-a095-4041-8d19-5f12500608a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = calculate_accuracy(preds, train_y_expanded)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d76941-4ded-4b58-b38e-c1235b94d923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastaicourse22p2v)",
   "language": "python",
   "name": "fastaicourse22p2v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
